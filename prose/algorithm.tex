\section{Deductive Search}
\label{sec:prose:algorithm}

A set of witness functions for all the parameters of an operator allows us to reduce the inductive synthesis problem
$\langle N, \spec\rangle$ to the synthesis subproblems for its parameters.
We introduce a simple non-conditional case first, and then proceed to complete presentation of the entire algorithm.

\begin{theorem}
    Let $N := F(N_1, \dots, N_k)$ be a rule in a DSL $\dsl$, and $\spec$ be a spec on $N$.
    Assume that $F$ has $k$ non-conditional witness functions $\omega_j(\spec) = \spec_j$,
    and $\vsa_j \models \spec_j$ for all $j = 1..k$ respectively.
    \begin{enumerate}[nosep]
        \item $\mathsf{Filter}(\joinCons(\vsa_1, \dots, \vsa_k), \spec) \models \spec$.
        \item If all $\omega_j$ are precise, then $\joinCons(\vsa_1, \dots, \vsa_k) \models \spec$.
    \end{enumerate}
    \label{thm:wf:noncond}
\end{theorem}
\begin{proof} \leavevmode
    \begin{enumerate}[nosep]
        \item By definition of $\mathsf{Filter}(\vsa, \spec)$.
        \item All $\omega_j$ deduce specs for $N_j$ given only the outer spec $\spec$, therefore they
            are independent from each other.
            Also, all $\omega_j$ are precise, therefore each $\vsa_j$ individually is necessary and sufficient to
            satisfy $\spec$. \qedhere
    \end{enumerate}
\end{proof}

\Cref{thm:wf:noncond} gives a straightforward recipe for synthesis of operators with independent parameters, such as
\dslinline|Pair($p_1$, $p_2$)|.
However, in most real-life cases operator parameters are dependent on each other.
Consider an operator \dslinline|Concat($atom$, $transform$)| from FlashFill, and a spec $\state \tospec s$.
It is possible to design individual witness functions $\omega_a$ and $\omega_t$ that return a disjunction $\spec_a$ of
prefixes of $s$ and a disjunction $\spec_t$ of suffixes of $s$, respectively.
Both of these witness functions individually are precise (i.e. sound and complete); however, there is no straightforward
way to combine recursive synthesis results $\vsa_a \models \spec_a$ and $\vsa_t \models \spec_t$ into a valid program
set for $\spec$.

In order to enable deductive search for dependent operator parameters, we apply \emph{skolemization}~\cite{modeltheory}.
Instead of deducing specs $\spec_a$ and $\spec_t$ that independently entail $\spec$, we deduce only one
independent spec (say, $\spec_a$), and then \emph{fix the value of ``$atom$''}.
For each fixed value of $atom$ a \emph{conditional witness function} $\omega_t(\spec \assuming atom = v)$ deduces a
spec $\spec_{t,v}$ that is a necessary and sufficient characterization for $\spec$.
Namely, $\spec_{t,v}$ in our example is $\state \tospec s[|v|..]$ (i.e. the remaining suffix) if $v$ is a prefix of~$s$,
or $\bot$ otherwise.

Skolemization splits the deduction into multiple independent branches, one per each value of $atom$.
These values are determined by VSA clustering: \mbox{$\clustering[\vsa_a] = \{v_1 \mapsto \vsa_a^1,
\dots, v_k \mapsto \vsa_a^k\}$}.
Within each branch, the program sets $\vsa_a^j$ and the corresponding $\vsa_t^j \models \spec_{t,v_j}$ are
independent, hence $\joinCons[\mathsf{Concat}](\vsa_a^j, \vsa_t^j) \models \spec$ by \Cref{thm:wf:noncond}.
The union of $k$ branch results constitutes a comprehensive set of all $\mathsf{Concat}$ programs that satisfy $\spec$.

\begin{defn}
    Let $N := F(N_1, \dots, N_k)$ be a rule in a DSL $\dsl$ with $k$ associated (possibly conditional) witness functions
    $\omega_1, \dots, \omega_k$.
    A \emph{dependency graph} of witness functions of $F$ is a directed graph $G(F) = \langle V, E\rangle$ where $V=
    \left\{N_1, \dots, N_k\right\}$, and $\langle N_i, N_j\rangle \in E$ iff $N_i$ is a prerequisite for $N_j$.
\end{defn}

A dependency graph can be thought of as a union of all possible Bayesian networks over parameters of $F$.
It is not a single Bayesian network because $G(F)$ may contain cycles: it is often possible to independently express
$N_i$ in terms
of $N_j$ as a witness function $w_i(\spec \assuming N_j = v)$ and $N_j$ in terms of $N_i$ as a different witness
function $w_j(\spec \assuming N_i = v)$.
One simple example of such phenomenon is \dslinline|Concat($atom$, $transform$)|: we showed above how to decompose its
inverse semantics into a witness function for prefixes and a conditional witness function for the suffix, but a
symmetrical decomposition into a witness function for suffixes and a conditional witness function for prefixes is also
possible.

\begin{figure}[p!]
    \begin{fullpage}
        \small
        \uwsinglespace
        \begin{algorithmic}[1]
            \Given{G(F)}{dependency graph of witness functions for the rule $F$}
            \Given{\spec}{specification for the rule $F$}
            \Functionx{LearnRule}{$G(F), \spec$}
            \State Permutation $\constraint \gets \mathsf{TopologicalSort}(G(F))$
            \State $\vsa \gets \bigvsaunion \bigl\{ \vsa' \bigmid \vsa' \in \Call{LearnPaths}{G(F), \spec, \constraint, 1, \varnothing} \bigr\}$
            \If{all witness functions in $G(F)$ are precise}
            \State \Return $\vsa$
            \Else
            \State \Return $\mathsf{Filter}(\vsa, \spec)$
            \EndIf
            \EndFunction
            \Statex
            \Given{\constraint}{permutation of the parameters of $F$}
            \Given{i}{index of a current deduced parameter in $\constraint$}
            \Given{Q}{a mapping of prerequisite values $\values_{k}$ and corresponding learnt program sets $\vsa_{k}$ on the current path}
            \Functionx{LearnPaths}{$G(F), \spec, \constraint, i, Q$}
            \If{$i > k$}
            \State Let $\vsa_1, \dots, \vsa_k$ be learnt program sets for $N_1, \dots, N_k$ in $Q$
            \State \Return $\left\{ \joinCons(\vsa_1, \dots, \vsa_k) \right\}$
            \EndIf
            \State $p \gets \constraint_i$ \Comment{Current iteration deduces the rule parameter $N_p$}
            \State Let $\omega_{p}(\spec \assuming N_{k_1} = \values_1, \dots, N_{k_m} = \values_m)$ be the witness
            function for $N_{p}$
            \Statex \Comment{Extract the prerequisite values for $N_{p}$ from the mapping $Q$}
            \State $\{\values_{k_1} \mapsto \vsa_{k_1}, \dots, \values_{k_m} \mapsto \vsa_{k_m}\} \gets Q[k_1, \dots, k_m]$
            \Statex \Comment{Deduce the spec for $N_{p}$ given $\spec$ and the prerequisites}
            \State $\spec_{p} \gets \omega_{p}(\spec \assuming N_{k_1} = \values_{k_1}, \dots, N_{k_m} = \values_{k_m})$ \label{alg:line:wf}
            \If{$\omega_p = \bot$}
            \Return $\emptyset$
            \EndIf
            \Statex \Comment{Recursively learn a valid program set $\vsa_{p} \models \spec_{p}$}
            \State $\vsa_{p} \gets \mathsf{Learn}(N_{p}, \spec_{p})$ \label{alg:line:sublearn}
            \Statex \Comment{If no other parameters depend on $N_{p}$, proceed without clustering}
            \If{$N_{p}$ is a leaf in $G(F)$}
            \State $Q' \gets Q[p := \top \mapsto \vsa_{p}]$
            \State \Return \Call{LearnPaths}{$G(F), \spec, \constraint, i+1, Q'$}
            \Statex \Comment{Otherwise cluster $\vsa_{p}$ on $\states$ and unite the results across branches}
            \Else
            \State $\states \gets$ the input states associated with $\spec$
            \ForAll{$\bigl(\values'_j \mapsto \vsa'_{s,j}\bigr) \in \clustering[\vsa_p][\states]$} \label{alg:line:clusters}
            \State $Q' \gets Q\bigl[s := \values'_j \mapsto \vsa'_{s,j}\bigr]$
            \State \Yield \textbf{all} \Call{LearnPaths}{$G(F), \spec, \constraint, i+1, Q'$}
            \EndFor
            \EndIf
            \EndFunction
        \end{algorithmic}
        % \end{mdframed}
        \caption{A learning procedure for the DSL rule $N := F(N_1, \dots, N_k)$ that uses $k$ conditional witness functions
        for $N_1, \dots, N_k$, expressed as a dependency graph $G(F)$.}
        \label{fig:prose:algorithm}
    \end{fullpage}
\end{figure}

\begin{theorem}
    Let $N := F(N_1, \dots, N_k)$ be a rule in a DSL $\dsl$, and $\spec$ be a spec on $N$.
    If there exists an acyclic spanning subgraph of $G(F)$ that includes each node with all its prerequisite edges, then
    there exists a polynomial procedure that constructs a valid program set $\vsa \models \spec$ from the valid
    parameter program sets $\vsa_j \models \spec_j$ for some choice of parameter specifications $\spec_j$.
    \label{thm:wf:cond}
\end{theorem}
\begin{proof}
    We define the learning procedure for $F$ in \Cref{fig:prose:algorithm} algorithmically.
    It recursively explores the dependency graph $G(F)$ in a topological order, maintaining a \emph{prerequisite path}
    $Q$ -- a set of
    parameters $N_j$ that have already been skolemized, together with their fixed bindings~$\values_j$ and valid program
    sets $\vsa_j$.
    In the prerequisite path, we maintain the invariant: \emph{for each program set $\vsa_j$ in the path, all programs
        in it produce the same values $\values_j$ on the provided input states~$\states$}.
    This allows each conditional witness function $\omega_{i}$ to deduce a spec $\spec_i$ for the current
    parameter~$N_i$ assuming the bound values $\values_{k_1}, \dots, \values_{k_s}$ for the prerequisites
    $N_{k_1}, \dots, N_{k_s}$ of $N_i$.

    The program sets in each path are valid for the subproblems deduced by applying witness functions.
    If all the witness functions in $G(F)$ are precise, then any combination of programs $P_1, \dots, P_k$ from these
    program sets yields a valid program $F(P_1, \dots, P_k)$ for $\spec$.
    If some witness functions are imprecise, then a filtered join of parameter program sets for each path is valid
    for $N$.
    Thus, the procedure in \Cref{fig:prose:algorithm} computes a valid program set $\vsa \models \spec$.
\end{proof}

\Cref{thm:wf:noncond,thm:wf:cond} give a \emph{constructive} definition of the refinement procedure that splits the
search space for $N$ into smaller parameter search spaces for $N_1,\dots,N_k$.
If the corresponding witness functions are precise, then \emph{every} combination of valid parameter programs from these
subspaces yields a valid program for the original synthesis problem.
Alternatively, if some of the accessible witness functions are imprecise, we use them to \emph{narrow down} the
parameter search space, and filter the constructed program set for validity.
The $\mathsf{Filter}$ operation (defined in \Cref{ch:vsa}) filters out inconsistent programs from $\vsa$ in time
proportional to $\clustering$.

\begin{sidewaysfigure}[p!]
    \begin{fullpage}
        \centering
        \input{algorithm-example}
        \large
        \vspace*{-\baselineskip}
        \begin{equation*}
            \spec\colon\ \stringliteral{(202) 555-0126} \tospec \stringliteral{202}
            \quad
            \spec_1\colon\ \stringliteral{(202) 555-0126} \tospec \stringliteral{2} \,\vee\, \stringliteral{20}
            \quad
            \begin{aligned}[t]
                &\spec_{21}\colon\ \stringliteral{(202) 555-0126} \tospec \stringliteral{02} \\[-0.3\baselineskip]
                &\spec_{22}\colon\ \stringliteral{(202) 555-0126} \tospec \stringliteral{2}
            \end{aligned}
        \end{equation*}
        \caption{An illustrative diagram showing the outermost layer of deductive search for a given problem
        $\mathsf{Learn}\left(\mathtt{transform}\,,\, \stringliteral{(202) 555-0126} \tospec \stringliteral{202}\right)$.
        Solid blue arrows show the recursive calls of the search process (the search subtrees below the outermost layers
        are not expanded and shown as ``\dots'').
        Rounded orange blocks and arrows shows the nested iterations of the \textsc{LearnPaths} procedure from
        \Cref{fig:prose:algorithm}.
        Dotted green arrows show the VSA structure that is returned to the caller.}
        \label{fig:prose:algorithm:example}
    \end{fullpage}
\end{sidewaysfigure}

\begin{example}
    \Cref{fig:prose:algorithm:example} shows an illustrative diagram for the outermost layer of the learning process for
    the \dslinline|$transform$| symbol from $\ffdsl$ and a given spec $\spec = \stringliteral{(202) 555-0126} \tospec
    \stringliteral{202}$.
    First, the framework splits the search process into two branches: one for \dslinline|Concat($atom$, $transform$)|
    and one for \dslinline|$atom$|, according to the definition of \dslinline|$transform$| in $\ffdsl$.
    The figure shows the outermost layer of the first branch.

    The \textsc{LearnPaths} procedure first invokes the witness function $\omega_a$ for the first
    parameter $atom$ of the \dslinline|Concat| rule.
    It returns a necessary spec $\spec_1 = \state \tospec \stringliteral{2} \,\vee\,
    \stringliteral{20}$ for the $atom$ symbol (where $\state$ is the shared input state from $\spec$).
    The PROSE framework then recursively resolves this spec into a VSA $\vsa_1$ of all $atom$ programs that satisfy
    $\spec_1$.

    Next, the \textsc{LearnPaths} procedure clusters $\vsa_1$ and splits its further execution into two branches, one
    per each cluster of programs in $\vsa_1$ that give the same output on the given input state $\state$.
    There are two clusters: programs $\vsa'_1$ that return $o_1 = \stringliteral{2}$ and programs $\vsa'_2$ that return
    $o_2 = \stringliteral{20}$.
    For each of them the PROSE framework independently invokes a nested call to \textsc{LearnPaths} with the
    corresponding output binding for $atom$ (i.e. $o_1$ or $o_2$ respectively) recorded in the prerequisite path $Q$.
    Within each nested invocation, \textsc{LearnPaths} constructs a necessary and sufficient spec on the second
    parameter $transform$ of \dslinline|Concat| by invoking its conditional witness function $\omega_t$.
    It returns a spec $\spec_{21}\colon\ \state \tospec \stringliteral{02}$ for the branch with $o_1 =
    \stringliteral{2}$ and a spec $\spec_{22}\colon\ \state \tospec \stringliteral{2}$ for the branch with $o_2 =
    \stringliteral{20}$, respectively.
    The framework then recursively resolves them into the corresponding program sets $\vsa_{21}$ and $\vsa_{22}$.

    Since both witness functions $\omega_a$ and $\omega_t$ are precise, the final result returned from
    \textsc{LearnPaths} is the VSA $\bigvsaunion\left\{ \joinCons[\mathsf{Concat}]\left(\vsa'_1,\, \vsa_{21}\right),\,
    \joinCons[\mathsf{Concat}]\left(\vsa'_2,\, \vsa_{22}\right) \right\}$.
\end{example}

\begin{figure}[t]
    \centering
    \small
    \uwsinglespace
    \begin{subfigure}[t]{\textwidth}
        \subcaptionbox{\label{fig:topdown:spec}}{\hphantom{(a)}}\vspace{-1.5\baselineskip}
        \begin{mathpar}
            \infer{ N := F_1(N_1, \dots, N_k) \;|\; F_2(M_1, \dots, M_n) \\\\
            \text{\textsc{LearnRule}}(G(F_1), \spec) = \vsa_1\ \quad\ \text{\textsc{LearnRule}}(G(F_2), \spec) = \vsa_2}
            {\mathsf{Learn}(N, \spec) = \vsa_1 \vsaunion \vsa_2}
            \quad\infer{  }{\mathsf{Learn}(N, \top) = \dsl|_N}
            \\ \infer
            {\forall j = 1..2\colon \mathsf{Learn}(N, \spec_j) = \vsa_j}
            {\mathsf{Learn}(N, \spec_1 \wedge \spec_2) = \vsa_1 \cap \vsa_2}
            \quad \infer
            {\forall j = 1..2\colon \mathsf{Learn}(N, \spec_j) = \vsa_j}
            {\mathsf{Learn}(N, \spec_1 \vee \spec_2) = \vsa_1 \vsaunion \vsa_2}
            \quad\infer{  }{\mathsf{Learn}(N, \spec) = \mathsf{Filter}(\dsl|_N, \spec)}
            \\ \infer
            {\mathsf{Learn}(N, \spec_1) = \vsa \quad \spec_2 = \neg \langle \state, \constraint\rangle}
            {\mathsf{Learn}(N, \spec_1 \wedge \spec_2) = \mathsf{Filter}(\vsa, \spec_2)}
            \quad \infer
            {N := F(N_1, \dots, N_k) \\\\ \text{All witness functions in $G(F)$ accept $\spec$}}
            {\mathsf{Learn}(N, \spec) = \text{\textsc{LearnRule}}(G(F), \spec)} \\
        \end{mathpar}
    \end{subfigure}
    \begin{subfigure}[t]{\textwidth}
        \subcaptionbox{\label{fig:wf:syntax}}{\hphantom{(b)}}\vspace{-\baselineskip}
        \begin{mathpar}
            \quad\infer
            {N := \text{let } x = e_1 \text{ in } e_2}
            {\omega_{e_2}(\spec \assuming e_1 = v) = \state[x := v] \rightsquigarrow \spec}
            \quad\infer
            {N \text{ is a literal }}
            {\mathsf{Learn}(N, \state \rightsquigarrow v) = \left\{v\right\}}
            \\ \infer
            {N \text{ is a variable}}
            {\mathsf{Learn}(N, \state \rightsquigarrow \constraint) = \left\{N\right\} \text{if } \constraint(\state[N]) \text{ else } \emptyset}
        \end{mathpar}
    \end{subfigure}
    \caption{\textbf{(a)} Constructive inference rules for processing of boolean connectives in the inductive specifications $\spec$;
    \textbf{(b)} Witness functions and inference rules for common syntactic features of PROSE DSLs: \texttt{let} definitions, variables, and literals.}
    \label{fig:topdown:common}
\end{figure}

\paragraph{Handling Boolean Connectives}
Witness functions for DSL operators (such as the ones in \Cref{tbl:wfs:flashfill}) are typically defined on the
\emph{atomic} constraints (such as equality or subsequence predicates).
To complete the definition of deductive search, \Cref{fig:topdown:spec} gives inference rules for handling
of boolean connectives in a spec $\spec$.
Since a spec is defined as a NNF, we give the rules for handling conjunctions and disjunctions of specs, and
positive/negative literals.
These rules directly map to corresponding VSA operations:

\begin{theorem} \leavevmode
    \begin{enumerate}[nosep, label=(\arabic*)]
        \item $\vsa_1 \models \spec_1$ and $\vsa_2 \models \spec_2$ $\ \Longleftrightarrow\ $ $\vsa_1 \vsaunion \vsa_2 \models \spec_1 \vee \spec_2$.
        \item $\vsa_1 \models \spec_1$ and $\vsa_2 \models \spec_2$ $\ \Longleftrightarrow\ $ $\vsa_1 \cap \vsa_2 \models \spec_1 \wedge \spec_2$.
        \item $\vsa \models \spec_1$ $\ \Longleftrightarrow\ $ $\mathsf{Filter}(\vsa, \spec_2) \models \spec_1 \wedge \spec_2$.
    \end{enumerate}
    \label{thm:connectives:vsa}
\end{theorem}

Handling negative literals is more difficult.
They can only be efficiently resolved in two cases:
\begin{enumerate*}[label=(\alph*)]
    \item if a witness function supports the negated spec directly, or
    \item if the negative literal occurs in a conjunction with a positive literal, in which case we use the latter to
        generate a base set of candidate programs, which is then filtered to also satisfy the former
\end{enumerate*}.
If neither (a) nor (b) holds, the set of programs satisfying a negative literal is bounded only by the DSL.

% Our pre-defined generic witness functions in \Cref{fig:wf:library}, together with witness functions for common syntactic
% features of PROSE DSLs (\texttt{let} rules, variables, and literals) constitute the PROSE standard library.

\paragraph{Search Tactics}
\Cref{thm:wf:cond,thm:connectives:vsa,fig:topdown:common} entail a non-deterministic choice among numerous possible ways
to explore the program space deductively.
For instance, one can have many different witness functions for the same operator $F$ in $G(F)$, and they may deduce subproblems of
different complexity.
A specific exploration choice in the program space constitutes a \emph{search tactic} over a DSL.
We have identified several effective generic search tactics, with different advantages and disadvantages;
however, a comprehensive study on their selection is left for future work.

Consider a conjunctive problem $\mathsf{Learn}(N, \spec_1 \wedge \spec_2)$.
One possible way to solve it is given by \Cref{thm:connectives:vsa}: handle two conjuncts independently, producing VSAs
$\vsa_1$ and $\vsa_2$, and intersect them.
This approach has a drawback: the complexity of VSA intersection is quadratic.
Even if $\spec_1$ and $\spec_2$ are inconsistent (i.e. $\vsa_1 \cap \vsa_2 = \emptyset$), each conjunct individually may
be satisfiable.
In this case the unsatisfiability of the original problem is determined only after $T(\mathsf{Learn}(N, \spec_1)) +
T(\mathsf{Learn}(N, \spec_2)) + \mathcal{O}(\volume{\vsa_1} \cdot \volume{\vsa_2})$ time.

An alternative search tactic for conjunctive specs arises when $\spec_1$ and $\spec_2$ constrain \emph{different} input
states $\state_1$ and $\state_2$, respectively.
In this case each conjunct represents an independent ``world'', and witness functions can deduce subproblems in each
``world'' independently and concurrently.
PROSE applies witness functions to each conjunct in the specification in parallel, conjuncts the resulting parameter
specs, and makes a single recursive learning call.
Such ``parallel processing'' of conjuncts in the spec continues up to the terminal level, where the deduced sets of
concrete values for each terminal are intersected across all input states.\footnote{
    The ``parallel'' approach can also be thought of as a deduction over a new isomorphic DSL,
    in which operators (and witness functions) are straightforwardly lifted to accept \emph{tuples} of values
    instead of single values.}

The main benefit of this approach is that unsatisfiable branches are eliminated much sooner.
For instance, if among $m$ I/O examples one example is inconsistent with the rest, a parallel approach
approach discovers it as soon as the relevant DSL level is reached, whereas an intersection-based approach has to first
construct $m$ VSAs (of which one is empty) and intersect them.
Its main disadvantage is that in presence of disjunction the number of branches grows exponentially in a number of input
states in the specification.

\paragraph{Optimizations}
PROSE performs many practical optimizations in the algorithm in \Cref{fig:prose:algorithm}.
We parallelize the loop in \Cref{alg:line:clusters}, since it explores non-intersecting portions of the program space.
For ranked synthesis, we only calculate top $k$ programs for leaf nodes of $G(F)$, provided the ranking
function is monotonic.
We also cache synthesis results for every distinct learning subproblem $\langle N, \spec\rangle$, which makes
deductive search an instance of \emph{dynamic programming}.
This optimization is crucial for efficient synthesis of many common DSL operators, as we explain in more details in
\Cref{sec:prose:evaluation:casestudies}.

For bottom portions of the DSL we switch to enumerative
search~\cite{transit:protocols}, which in such conditions is more efficient than deduction, provided no constants need
to be synthesized.
In principle, every subproblem $\mathsf{Learn}(N, \spec)$ in PROSE can be solved by any sound strategy, not
necessarily deduction or enumerative search.
Possible alternatives include constraint solving or stochastic techniques~\cite{sygus}.
